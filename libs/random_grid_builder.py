# Databricks notebook source
class RandomGridBuilder: 
  '''Grid builder for random search. Sets up grids for use in CrossValidator in Spark using values randomly sampled from user-provided distributions.
  Distributions should be provided as lambda functions, so that the numbers are generated at call time.
  
  Parameters:
    num_models: Integer (Python) - number of models to generate hyperparameters for
    seed: Integer (Python) - seed (optional, default is None)
    
  Returns:
    param_map: list of parameter maps to use in cross validation.
    
  Example usage:
    from pyspark.ml.classification import LogisticRegression
    lr = LogisticRegression()
    paramGrid = RandomGridBuilder(2)\
               .addDistr(lr.regParam, lambda: np.random.rand()) \
               .addDistr(lr.maxIter, lambda : np.random.randint(10))\
               .build()
               
    Returns similar output as Spark ML class ParamGridBuilder and can be used in its place. The above paramGrid provides random hyperparameters for 2 models.
    '''
  
  def __init__(self, num_models, seed=None):
    self._param_grid = {}
    self.num_models = num_models
    self.seed = seed
    
  def addDistr(self, param, distr_generator):
    '''Add distribution based on dictionary generated by function passed to addDistr.'''
    
    if 'pyspark.ml.param.Param' in str(type(param)):
      self._param_grid[param] = distr_generator
    else:
      raise TypeError('param must be an instance of Param')

    return self
  
  def build(self):    
    param_map = []
    for n in range(self.num_models):
      if self.seed:
        # Set seeds for both numpy and random in case either is used for the random distribution
        np.random.seed(self.seed + n)
        random.seed(self.seed + n)
      param_dict = {}
      for param, distr in self._param_grid.items():
        param_dict[param] = distr()
      param_map.append(param_dict)
    
    return param_map

# COMMAND ----------

blob_container = "w261-scrr" # The name of your container created in https://portal.azure.com
storage_account = "midsw261rv" # The name of your Storage account created in https://portal.azure.com
secret_scope = "w261scrr" # The name of the scope created in your local computer using the Databricks CLI
secret_key = "w261scrrkey" # The name of the secret key created in your local computer using the Databricks CLI 
blob_url = f"wasbs://{blob_container}@{storage_account}.blob.core.windows.net"
mount_path = "/mnt/mids-w261"

import pyspark
from pyspark.sql.functions import col, concat, lit, regexp_replace, when, length, lpad, to_timestamp, mean, stddev
from pyspark.sql.types import StringType, BooleanType, IntegerType
import pyspark.sql.functions as F
from itertools import chain

import airporttime
from datetime import datetime, timedelta
import numpy as np
spark.conf.set(
  f"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net",
  dbutils.secrets.get(scope = secret_scope, key = secret_key)
)

df_joined = spark.read.parquet(f"{blob_url}/join_6m_0329")

# COMMAND ----------

from pyspark.ml.classification import RandomForestClassifier
import numpy as np
import math

clf = RandomForestClassifier(
                             featuresCol = 'features', 
                             labelCol = 'label'
                            )  

# Set up cross validation with grid search
randomParams = RandomGridBuilder(3)\
                 .addDistr(clf.featureSubsetStrategy, lambda : str(np.random.rand()))\
                 .addDistr(clf.maxDepth, lambda : np.random.randint(2, 24))\
                 .addDistr(clf.numTrees, lambda : math.floor(500*np.random.power(1)))\
                 .addDistr(clf.impurity, lambda : np.random.choice(['gini', 'entropy']))\
                 .build()

evaluator = BinaryClassificationEvaluator()

# Cross validator with random search
cv = [(sorted_groups[i] + sorted_groups[i+1], sorted_groups[i+2])
      for i in range(len(sorted_groups)-2)]

cv = CrossValidator(
                    estimator = clf, 
                    estimatorParamMaps = randomParams, 
                    evaluator = evaluator,   
                    parallelism = 2
                   )

cvModel = cv.fit(dataset)